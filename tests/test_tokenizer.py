from transformers import AutoTokenizer
from exllamav2 import ExLlamaV2Config
from exllamav2 import ExLlamaV2Tokenizer
import random

model_path = "/mnt/str/models/_exl2/deepseek-coder-1.3b"

config = ExLlamaV2Config()
config.model_dir = model_path
config.prepare()
exl2_tokenizer = ExLlamaV2Tokenizer(config)

reference_tokenizer = AutoTokenizer.from_pretrained(model_path)

# Bunch of text

text = """

Following added tokens are encoded correctly but decoded incorrectly by the HF tokenizer:

Ã¶
Ã·
Ã
Ã½
Ã€
Ã¿
Ã¸
Ãº
Ã¾
Ã¼
Ã¹
Ã¶
Ã»

(workaround seems to be working for now)



This command might take a bit of time if your corpus is very large, but for this dataset of 1.6 GB of texts itâ€™s blazing fast (1 minute 16 seconds on an AMD Ryzen 9
 3900X CPU with 12 cores). Note that AutoTokenizer.train_new_from_iterator() only works if the tokenizer you are using is a â€œfastâ€ tokenizer. As youâ€™ll see in the next section,
  the ğŸ¤— Transformers library contains two types of tokenizers: some are written purely in Python and others (the fast ones) are backed by the ğŸ¤— Tokenizers library, which is
   written in the Rust programming language. Python is the language most often used for data science and deep learning applications, but when anything needs to be parallelized
    to be fast, it has to be written in another language. For instance, the matrix multiplications that are at the core of the model computation are written in CUDA, an 
   optimized C library for GPUs.
Training a brand new tokenizer in pure Python would be excruciating
a language model is not available in the language you are interested in, or if your corpus is very different from the one your language model was trained on, you will most 
likely want to retrain the model from scratch using a tokenizer adapted to your data. That will require training a new tokenizer on your dataset. But what exactly does that
 mean? When we first looked at tokenizers in Chapter 2, we saw that most Transformer models use a subword tokenization algorithm. To identify which subwords are of interest
  and occur most frequently in the corpus at hand, the tokenizer needs to take a hard look at all the texts in the corpus â€” a process we call training. The exact rules that
   govern thi


!
!asdasd
!asdfasdas asd asd asd asd 
xx                                                                                              xxxx
#######################################################################################################
*âœ°
â‹šâŠ¹
(â•¬â½â½ â° â¾â¾ Ğ” â½â½ â° â¾â¾)
(â—Â´âŒ“`â—)
(ã‚·_ _)ã‚·
m(_ _)m
(ï¼¾-ï¼¾)ï¼¿æ—¥
((ìœ âˆ€ìœ |||))
(à¹‘Â´â€¢ .Ì« â€¢à¥‚`à¹‘)
.Â°(à²—Ğ´à²—ã€‚)Â°.
(Î˜ï¸¹Î˜)áƒ¡
â—(à¹‘ê’ªà»ˆà±ªÌ®ê’ªà»ˆà¹‘)â—œ
(á—’á—£á—•)Õ
à¥‚(ÊšÌ´Ì¶Ì·Ì .Ì  ÊšÌ´Ì¶Ì·Ì¥Ì€ à¥‚)
(â”³Ğ”â”³)
(áµ’Ì¤Ì‘ â‚€Ì‘ áµ’Ì¤Ì‘)wow
Ù©(âŒ¯ê’¦àº´Ì†áµ”ê’¦àº´)Û¶áµ’áµáµáµáµáµ
âŠ™â–‚âŠ™
(Â´âŠ™Ï‰
(â—Ï‰â—‘ )
áƒš(Ìâ—‰â—à±ªâ—Ÿâ—‰â€µáƒš)
(Â´Â°Ì¥Ì¥Ì¥Ì¥Ì¥Ì¥Ì¥Ì¥Ï‰Â°Ì¥Ì¥Ì¥Ì¥Ì¥Ì¥Ì¥Ì¥ï½€)
ê’°áƒ¦Ë˜â€¿Ë˜à®±ê’±â¤âƒ›
-(à¹‘â˜†â€¿ â˜†#)á•—
â¸‚â¸‚â¸œ(à´°á´—à´°à¹‘)â¸â¸ƒâ¸ƒ
Ù©(â€¢Ì¤Ì€áµ•â€¢Ì¤Ìà¹‘)áµ’áµáµáµáµáµ
(âœŒï¾Ÿâˆ€ï¾Ÿ)â˜
(à¸‡'Ì€-'Ì)à¸‡
( Ë˜â–½Ë˜)ã£â™¨

ly slow, which is why we developed the ğŸ¤— Tokenizers library. Note that just as you didnâ€™t have to learn the CUDA language to be able to execute your model on a batch of i
nputs on a GPU, you wonâ€™t need to learn Rust to use a fast tokenizer. The ğŸ¤— Tokenizers library provides Python bindings for many methods that internally call some piece o
f code in Rust; for example, to parallelize the training of your new tokenizer or, as we saw in Chapter 3, the tokenization of a batch of inputs.
Most of the Transformer models have a fast tokenizer available (there are some exceptions that you can check here), and the AutoTokenizer API always selects the 
fast tokenizer for you if itâ€™s available. In the next section weâ€™ll take a look at some of the other special features fast tokenizers have, which will be really useful f
or tasks like token classification and question answering. Before diving into that, however, letâ€™s try our brand new tokenizer on the previous example:
tokens = tokenizer.tokenize(example)
tokens
(ËµÂ¯Í’ã€°Â¯Í’Ëµ)
à³­à©§(â›ã€œâ›âœ¿)à©­à³¨
(,,â—•ã€€â‹ã€€â—•,,)
( Ø•Ø”Ê˜Ì¥Ì¥Ì¥Ì¥ Ù‡ Ø”Ø•Ê˜Ì¥Ì¥Ì¥Ì¥ )?
à² _à²°à³ƒ
â™¥â•£[-_-]â• â™¥
(âˆ¿Â°â—‹Â°)âˆ¿ ï¸µ ÇÊŒol
ï¼ˆã€‚Ë‡ âŠ–Ë‡ï¼‰â™¡
â€§âºâ—Ÿ( áµ’Ì´Ì¶Ì·Ì¥Ì Â·Ì« áµ’Ì´Ì¶Ì·Ì£Ì¥Ì€ )
ê’°âŒ—Â´Íˆ áµ• à¥£`ÍˆâŒ—ê’±à§©
(áµ’Ì´Ì¶Ì·Ì¤Ìâ—à±ªâ—Ÿ áµ’Ì´Ì¶Ì·Ì¤Ì€ )

['def', 'Ä add', '_', 'numbers', '(', 'a', ',', 'Ä b', '):', 'ÄŠÄ Ä Ä ', 'Ä , 'Add', 'Ä the', 'Ä two', 'Ä numbers', 'Ä `',
 'a', '`', 'Ä and', 'Ä `', 'b', '`.'ÄŠÄ Ä Ä ', 'Ä return', 'Ä a', 'Ä +', 'Ä b']
Here we again see the special symbols Ä  and ÄŠ that denote spaces and newlines, but we can also see that our tokenizer learned some tokens that are highly specific to 
a corpus of
ä½¿ç”¨å½“ä»Šæœ€å¸¸ç”¨çš„åˆ†è¯å™¨æ¥è®­ç»ƒæ–°è¯æ±‡å¹¶è¿›è¡Œåˆ†è¯ã€‚
     ç”±äº Rust å®ç°ï¼Œé€Ÿåº¦éå¸¸å¿«ï¼ˆè®­ç»ƒå’Œæ ‡è®°åŒ–ï¼‰ã€‚ åœ¨æœåŠ¡å™¨ CPU ä¸Šæ ‡è®° 1 GB æ–‡æœ¬åªéœ€ä¸åˆ° 20 ç§’ã€‚
     æ˜“äºä½¿ç”¨ï¼Œè€Œä¸”ç”¨é€”æå…¶å¹¿æ³›ã€‚
     ä¸“ä¸ºç ”ç©¶å’Œç”Ÿäº§è€Œè®¾è®¡ã€‚
     æ ‡å‡†åŒ–ä¼´éšç€å¯¹é½è·Ÿè¸ªã€‚ æ€»æ˜¯å¯ä»¥è·å¾—åŸå§‹å¥å­ä¸­ä¸ç»™å®šæ ‡è®°ç›¸å¯¹åº”çš„éƒ¨åˆ†ã€‚
     æ‰§è¡Œæ‰€æœ‰é¢„å¤„ç†ï¼šæˆªæ–­ã€å¡«å……ã€æ·»åŠ æ¨¡å‹æ‰€éœ€çš„ç‰¹æ®Šæ ‡è®°ã€‚
ä½¿ç”¨ç•¶ä»Šæœ€å¸¸ç”¨çš„åˆ†è©å™¨ä¾†è¨“ç·´æ–°è©å½™ä¸¦é€²è¡Œåˆ†è©ã€‚
     ç”±æ–¼ Rust å¯¦ç¾ï¼Œé€Ÿåº¦éå¸¸å¿«ï¼ˆè¨“ç·´å’Œæ¨™è¨˜åŒ–ï¼‰ã€‚ åœ¨ä¼ºæœå™¨ CPU ä¸Šæ¨™è¨˜ 1 GB æ–‡å­—åªéœ€ä¸åˆ° 20 ç§’ã€‚
     æ˜“æ–¼ä½¿ç”¨ï¼Œè€Œä¸”ç”¨é€”æ¥µç‚ºå»£æ³›ã€‚
     å°ˆç‚ºç ”ç©¶å’Œç”Ÿç”¢è€Œè¨­è¨ˆã€‚
     æ¨™æº–åŒ–ä¼´éš¨è‘—å°é½Šè¿½è¹¤ã€‚ ç¸½æ˜¯å¯ä»¥ç²å¾—åŸå§‹å¥å­ä¸­èˆ‡çµ¦å®šæ¨™è¨˜ç›¸å°æ‡‰çš„éƒ¨åˆ†ã€‚
     åŸ·è¡Œæ‰€æœ‰é è™•ç†ï¼šæˆªæ–·ã€å¡«å……ã€æ–°å¢æ¨¡å‹æ‰€éœ€çš„ç‰¹æ®Šæ¨™è¨˜ã€‚
     IÅ¡mokykite naujus Å¾odynus ir Å¾etonus naudodami Å¡iandien daÅ¾niausiai naudojamus Å¾etonus.
     Itin greitas (ir mokymas, ir tokenizavimas), dÄ—ka Rust Ä¯gyvendinimo. GB teksto atpaÅ¾inimas serverio procesoriuje trunka maÅ¾iau nei 20 sekundÅ¾iÅ³.
     Lengva naudoti, bet ir itin universalus.
     Sukurta tyrimams ir gamybai.
     Normalizacija ateina su derinimo stebÄ—jimu. Visada galima gauti pradinio sakinio dalÄ¯, atitinkanÄiÄ… duotÄ… Å¾etonÄ….
     Atlieka visÄ… iÅ¡ankstinÄ¯ apdorojimÄ…: Sutrumpinkite, Pad, pridÄ—kite specialius Å¾etonus, kuriÅ³ reikia jÅ«sÅ³ modeliui.
ì˜¤ëŠ˜ë‚  ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ ì–´íœ˜ë¥¼ í›ˆë ¨í•˜ê³  í† í°í™”í•©ë‹ˆë‹¤.
     Rust êµ¬í˜„ ë•ë¶„ì— ë§¤ìš° ë¹ ë¦…ë‹ˆë‹¤(í›ˆë ¨ ë° í† í°í™” ëª¨ë‘). ì„œë²„ CPUì—ì„œ 1GBì˜ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ëŠ” ë° 20ì´ˆë„ ì±„ ê±¸ë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤.
     ì‚¬ìš©í•˜ê¸° ì‰½ì§€ë§Œ ë§¤ìš° ë‹¤ì–‘í•©ë‹ˆë‹¤.
     ì—°êµ¬ ë° ìƒì‚°ìš©ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
     ì •ê·œí™”ì—ëŠ” ì •ë ¬ ì¶”ì ì´ í¬í•¨ë©ë‹ˆë‹¤. ì£¼ì–´ì§„ í† í°ì— í•´ë‹¹í•˜ëŠ” ì›ë˜ ë¬¸ì¥ì˜ ë¶€ë¶„ì„ ì–»ëŠ” ê²ƒì€ í•­ìƒ ê°€ëŠ¥í•©ë‹ˆë‹¤.
     ìë¥´ê¸°, ì±„ìš°ê¸°, ëª¨ë¸ì— í•„ìš”í•œ íŠ¹ìˆ˜ í† í° ì¶”ê°€ ë“± ëª¨ë“  ì‚¬ì „ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
     ĞĞ·Ñ‹Ñ€ĞºÑ‹ ÑÒ£ ĞºÓ©Ğ¿ ĞºĞ¾Ğ»Ğ´Ğ¾Ğ½ÑƒĞ»Ğ³Ğ°Ğ½ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ»Ğ¾Ñ€Ğ´Ñƒ ĞºĞ¾Ğ»Ğ´Ğ¾Ğ½ÑƒĞ¿, Ğ¶Ğ°Ò£Ñ‹ Ğ»ĞµĞºÑĞ¸ĞºĞ°Ğ»Ğ°Ñ€Ğ´Ñ‹ Ò¯Ğ¹Ñ€Ó©Ñ‚Ò¯Ò£Ò¯Ğ· Ğ¶Ğ°Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ»Ğ°Ò£Ñ‹Ğ·.
     Rust Ğ¸ÑˆĞºĞµ Ğ°ÑˆÑ‹Ñ€ÑƒÑƒĞ½ÑƒĞ½ Ğ°Ñ€ĞºĞ°ÑÑ‹Ğ½Ğ´Ğ° Ğ°Ğ±Ğ´Ğ°Ğ½ Ñ‚ĞµĞ· (Ğ¾ĞºÑƒÑ‚ÑƒÑƒ Ğ¶Ğ°Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ). Ğ¡ĞµÑ€Ğ²ĞµÑ€Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€ÑƒĞ½Ğ´Ğ° Ğ“Ğ‘ Ñ‚ĞµĞºÑÑ‚Ñ‚Ğ¸ Ğ±ĞµĞ»Ğ³Ğ¸Ğ»Ó©Ó© Ò¯Ñ‡Ò¯Ğ½ 20 ÑĞµĞºÑƒĞ½Ğ´Ğ´Ğ°Ğ½ Ğ°Ğ· ÑƒĞ±Ğ°ĞºÑ‹Ñ‚ ĞºĞµÑ‚ĞµÑ‚.
     ĞšĞ¾Ğ»Ğ´Ğ¾Ğ½ÑƒÑƒ Ğ¾Ò£Ğ¾Ğ¹, Ğ±Ğ¸Ñ€Ğ¾Ğº Ğ¾ÑˆĞ¾Ğ½Ğ´Ğ¾Ğ¹ ÑĞ»Ğµ Ó©Ñ‚Ó© Ğ°Ñ€ Ñ‚Ğ°Ñ€Ğ°Ğ¿Ñ‚ÑƒÑƒ.
     Ğ˜Ğ·Ğ¸Ğ»Ğ´Ó©Ó© Ğ¶Ğ°Ğ½Ğ° Ó©Ğ½Ğ´Ò¯Ñ€Ò¯Ñˆ Ò¯Ñ‡Ò¯Ğ½ Ğ¸ÑˆÑ‚ĞµĞ»Ğ¸Ğ¿ Ñ‡Ñ‹ĞºĞºĞ°Ğ½.
     ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ´Ğ°ÑˆÑ‚Ñ‹Ñ€ÑƒÑƒ Ñ‚ĞµĞ³Ğ¸Ğ·Ğ´Ó©Ó©Ğ»Ó©Ñ€Ğ´Ò¯ ĞºÓ©Ğ·Ó©Ğ¼Ó©Ğ»Ğ´Ó©Ó© Ğ¼ĞµĞ½ĞµĞ½ ĞºĞµĞ»ĞµÑ‚. Ğ‘ĞµÑ€Ğ¸Ğ»Ğ³ĞµĞ½ Ğ±ĞµĞ»Ğ³Ğ¸Ğ³Ğµ Ñ‚ÑƒÑƒÑ€Ğ° ĞºĞµĞ»Ğ³ĞµĞ½ Ğ±Ğ°ÑˆÑ‚Ğ°Ğ¿ĞºÑ‹ ÑÒ¯Ğ¹Ğ»Ó©Ğ¼Ğ´Ò¯Ğ½ Ğ±Ó©Ğ»Ò¯Ğ³Ò¯Ğ½ Ğ°Ğ»ÑƒÑƒ Ğ°Ñ€ Ğ´Ğ°Ğ¹Ñ‹Ğ¼ Ğ¼Ò¯Ğ¼ĞºÒ¯Ğ½.
     Ğ‘Ğ°Ñ€Ğ´Ñ‹Ğº Ğ°Ğ»Ğ´Ñ‹Ğ½ Ğ°Ğ»Ğ° Ğ¸ÑˆÑ‚ĞµÑ‚Ò¯Ò¯Ğ½Ò¯ Ğ°Ñ‚ĞºĞ°Ñ€Ğ°Ñ‚: ĞšÑ‹ÑĞºĞ°Ñ€Ñ‚Ñ‹Ò£Ñ‹Ğ·, Pad, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ò£Ğ¸Ğ·Ğ³Ğµ ĞºĞµÑ€ĞµĞºÑ‚Ò¯Ò¯ Ğ°Ñ‚Ğ°Ğ¹Ñ‹Ğ½ Ğ±ĞµĞ»Ğ³Ğ¸Ğ»ĞµÑ€Ğ´Ğ¸ ĞºĞ¾ÑˆÑƒÒ£ÑƒĞ·.
TrÃ¤na nya ordfxrrÃ¥d och tokenisera med dagens mest anvÃ¤nda tokenizers.
     Extremt snabb (bÃ¥de trÃ¤ning och tokenisering), tack vare Rust-implementeringen. Det tar mindre Ã¤n 20 sekunder att tokenisera en GB text pÃ¥ en servers CPU.
     LÃ¤tt att anvÃ¤nda, men ocksÃ¥ extremt mÃ¥ngsidig.
     Designad fxr forskning och produktion.
     Normalisering kommer med anpassningsspÃ¥rning. Det Ã¤r alltid mxjligt att fÃ¥ den del av den ursprungliga meningen som motsvarar en given token.
     Gxr all fxrbearbetning: Truncate, Pad, lÃ¤gg till de speciella tokens som din modell behxver.
                 out_q.device().is_meta() ? NULL : ((uint16_t*) out_q.data_ptr()) + c * columns,
            1,
            columns,
            qzero,
            maxq
        );

        adjust_error_row_cuda
        (
            (const float*) hessian_inv.data_ptr(),
            (float*) error.data_ptr(),
            (const float*) weights.data_ptr(),
            (const float*) quant.data_ptr(),
            c,
12345678912345678/92135678912315646845
364da
/................./////////
/* */"
" 2mkqwd "'2"'"'2'2'"@\\n\\n'#]

Let's write some cuneiform in a fixed-width font.
Normally, every character should line up with a character above.
Here: ğ’ˆ™. See how these characters don't align correctly?
Not only is it a very wide glyph, but its width is not even a multiple.
At least not in my font (Mac Safari 15.0).
But Ç„ is ok.


posted 14 years ago

    Mark post as helpful send pies Quote Report post to moderator 

Hi,
?
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
	
public static void main(String[] args) throws FileNotFoundException, IOException,
            UnsupportedRTFTemplate, Exception {
        System.setProperty("file.enconding", "ISO-8859-1");
        RTFTemplateBuilder builder = RTFTemplateBuilder.newRTFTemplateBuilder();
        RTFTemplate rtf = builder.newRTFTemplate();
        FileReader rtfSource = new FileReader("source.rtf");
        File rtfTarget = new File("target.rtf");
        copyFile(new File("source.rtf"), rtfTarget.getAbsolutePath());
        rtf.setTemplate(rtfSource);
        rtf.put("NAME", "Maiko Cezar");
        String x = new String("SÃ£o Paulo");
        System.out.println(x);
        x = new String(x.getBytes(), "ISO-8859-1");
        System.out.println(x);
        rtf.put("CITY", x);
        rtf.merge(rtfTarget);
    }


my output
SÃ£o Paulo
SÃƒÂ£o Paulo

And then I put this (SÃƒÂ£o Paulo) in the rtfTarget, but without successful. The "ISO-8859-1" encode it's the rtf native encode, isn't?

bb

ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚ğ’€‚

ğŸ˜€ 	ğŸ˜€ 	ğŸ˜€ 	â€” 	â€” 	â€” 	grinning face
2 	U+1F603 	ğŸ˜ƒ 	ğŸ˜ƒ 	ğŸ˜ƒ 	ğŸ˜ƒ 	ğŸ˜ƒ 	ğŸ˜ƒ 	grinning face with big eyes
3 	U+1F604 	ğŸ˜„ 	ğŸ˜„ 	ğŸ˜„ 	ğŸ˜„ 	â€” 	â€” 	grinning face with smiling eyes
4 	U+1F601 	ğŸ˜ 	ğŸ˜ 	ğŸ˜ 	ğŸ˜ 	ğŸ˜ 	ğŸ˜ 	beaming face with smiling eyes
5 	U+1F606 	ğŸ˜† 	ğŸ˜† 	ğŸ˜† 	â€” 	ğŸ˜† 	â€” 	grinning squinting face
6 	U+1F605 	ğŸ˜… 	ğŸ˜… 	ğŸ˜… 	â€” 	ğŸ˜… 	â€” 	grinning face with sweat
7 	U+1F923 	ğŸ¤£ 	ğŸ¤£ 	â€” 	â€” 	â€” 	â€” 	rolling on the floor laughing
8 	U+1F602 	ğŸ˜‚ 	ğŸ˜‚ 	ğŸ˜‚ 	ğŸ˜‚ 	â€” 	ğŸ˜‚ 	face with tears of joy
9 	U+1F642 	ğŸ™‚ 	ğŸ™‚ 	ğŸ™‚ 	â€” 	â€” 	â€” 	slightly smiling face
10 	U+1F643 	ğŸ™ƒ 	ğŸ™ƒ 	â€” 	â€” 	â€” 	â€” 	upside-down face
11 	U+1FAE0 	ğŸ«  	ğŸ«  	â€” 	â€” 	â€” 	â€” 	melting face
12 	U+1F609 	ğŸ˜‰ 	ğŸ˜‰ 	ğŸ˜‰

ğŸ˜‹ 	ğŸ˜‹ 	ğŸ˜‹ 	â€” 	ğŸ˜‹ 	â€” 	face savoring food
25 	U+1F61B 	ğŸ˜› 	ğŸ˜› 	â€” 	â€” 	â€” 	â€” 	face with tongue
26 	U+1F61C 	ğŸ˜œ 	ğŸ˜œ 	ğŸ˜œ 	ğŸ˜œ 	ğŸ˜œ 	ğŸ˜œ 	winking face with tongue
27 	U+1F92A 	ğŸ¤ª 	ğŸ¤ª 	â€” 	â€” 	â€” 	â€” 	zany face
28 	U+1F61D 	ğŸ˜ 	ğŸ˜ 	ğŸ˜ 	ğŸ˜ 	â€” 	â€” 	squinting face with tongue
29 	U+1F911 	ğŸ¤‘ 	ğŸ¤‘ 	â€” 	â€” 	â€”

24 	U+1F60B 	ğŸ˜‹ 	ğŸ˜‹ 	ğŸ˜‹ 	â€” 	ğŸ˜‹ 	â€” 	face savoring food
25 	U+1F61B 	ğŸ˜› 	ğŸ˜› 	â€” 	â€” 	â€” 	â€” 	face with tongue
26 	U+1F61C 	ğŸ˜œ 	ğŸ˜œ 	ğŸ˜œ 	ğŸ˜œ 	ğŸ˜œ 	ğŸ˜œ 	winking face with tongue
27 	U+1F92A 	ğŸ¤ª 	ğŸ¤ª 	â€” 	â€” 	â€” 	â€” 	zany face

"""


for i in range(2000):
    p = random.randint(0, len(text) - 10)
    l = random.randint(0, len(text) // 2)

    print(".", end="")

    chunk = text[p:p+l]
    x = reference_tokenizer.encode(chunk, add_special_tokens = False)
    y = exl2_tokenizer.encode(chunk)[0]
    if x != y.tolist():
        print("dammit")

    y_ = exl2_tokenizer.decode(y)
    if y_ != chunk:
        print("curses")
